{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import shutil\n",
    "import polars as pl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to where the original tar files are stored\n",
    "data_dir_bbo   = \"SP500_2010/bbo\"\n",
    "data_dir_trade = \"SP500_2010/trade\"\n",
    "\n",
    "\n",
    "extracted_base_dir = \"SP500_2010_extracted\"\n",
    "\n",
    "if not os.path.exists(extracted_base_dir):\n",
    "    os.makedirs(extracted_base_dir)\n",
    "\n",
    "# Gather up all .tar files\n",
    "bbo_tar_files = [\n",
    "    os.path.join(data_dir_bbo, f)\n",
    "    for f in os.listdir(data_dir_bbo)\n",
    "    if f.endswith(\".tar\")\n",
    "]\n",
    "\n",
    "trade_tar_files = [\n",
    "    os.path.join(data_dir_trade, f)\n",
    "    for f in os.listdir(data_dir_trade)\n",
    "    if f.endswith(\".tar\")\n",
    "]\n",
    "\n",
    "print(\"BBO tar files found:\", len(bbo_tar_files))\n",
    "print(\"Trade tar files found:\", len(trade_tar_files))\n",
    "\n",
    "if not full_analysis:\n",
    "    # randomly sample 10 files\n",
    "    bbo_tar_files = random.sample(bbo_tar_files, 10)\n",
    "    trade_tar_files = random.sample(trade_tar_files, 10)\n",
    "\n",
    "print(\"BBO tar files to process:\", len(bbo_tar_files))\n",
    "print(\"Trade tar files to process:\", len(trade_tar_files))\n",
    "\n",
    "if os.path.exists(\"SP500_2010\"):\n",
    "    shutil.rmtree(\"SP500_2010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_tar_to_ticker_subfolder(tar_path, parent_out_dir, subfolder_name):\n",
    "    \"\"\"\n",
    "    Extracts a .tar file into the directory structure:\n",
    "       parent_out_dir / <TICKER> / <subfolder_name> / <extracted CSV files>\n",
    "    \n",
    "    The TICKER is inferred from the tar file name (before .tar).\n",
    "    The subfolder_name is either 'bbo' or 'trade' (depending on the source).\n",
    "    \"\"\"\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(tar_path))[0]\n",
    "    \n",
    "    \n",
    "    ticker = base_name.split(\"-\")[0]\n",
    "    \n",
    "    # Create the directory for that ticker\n",
    "    ticker_dir = os.path.join(parent_out_dir, ticker)\n",
    "    if not os.path.exists(ticker_dir):\n",
    "        os.makedirs(ticker_dir)\n",
    "    \n",
    "    # Then within the ticker folder, create a subfolder named either \"bbo\" or \"trade\"\n",
    "    output_dir = os.path.join(ticker_dir, subfolder_name)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Extract the .tar\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if member.isfile():\n",
    "                # Avoid preserving deep internal paths by stripping them\n",
    "                member.name = os.path.basename(member.name)\n",
    "                tar.extract(member, output_dir)\n",
    "    \n",
    "    print(f\"Extracted {tar_path} -> {output_dir}\")\n",
    "\n",
    "\n",
    "# Extract BBO tar files\n",
    "for tar_file in bbo_tar_files:\n",
    "    extract_tar_to_ticker_subfolder(\n",
    "        tar_path=tar_file,\n",
    "        parent_out_dir=extracted_base_dir,\n",
    "        subfolder_name=\"bbo\"\n",
    "    )\n",
    "\n",
    "# Extract TRADE tar files\n",
    "for tar_file in trade_tar_files:\n",
    "    extract_tar_to_ticker_subfolder(\n",
    "        tar_path=tar_file,\n",
    "        parent_out_dir=extracted_base_dir,\n",
    "        subfolder_name=\"trade\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_dir = \"SP500_2010_extracted\"\n",
    "\n",
    "for root, dirs, files in os.walk(extracted_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".parquet\"):\n",
    "            parquet_path = os.path.join(root, file)\n",
    "            \n",
    "            # 1) Read the parquet file into a pandas DataFrame\n",
    "            df = pd.read_parquet(parquet_path)\n",
    "            \n",
    "            # 2) Construct the CSV name. For example, if the file is\n",
    "            \n",
    "            csv_name = file.replace(\".parquet\", \".csv\")\n",
    "            csv_path = os.path.join(root, csv_name)\n",
    "            \n",
    "            # 3) Save to CSV\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            \n",
    "            print(f\"Converted {parquet_path} -> {csv_path}\")\n",
    "            \n",
    "            \n",
    "            os.remove(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel's day offset to 1970-01-01\n",
    "EXCEL_EPOCH_OFFSET = 25569\n",
    "SECONDS_PER_DAY    = 86400\n",
    "\n",
    "# For May 2010, NY is UTC-4 (DST).\n",
    "UTC_OFFSET_HOURS   = 4  # subtract 4 hours\n",
    "\n",
    "# Directory containing your .csv files\n",
    "base_dir = \"SP500_2010_extracted\"\n",
    "\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(root, filename)\n",
    "            \n",
    "            # 1) Read with Polars\n",
    "            df = pl.read_csv(csv_path)\n",
    "\n",
    "            # 2) If \"xltime\" is present, convert it to local NY time\n",
    "            #    (one column: 'ny_timestamp') and drop \"xltime\"\n",
    "            if \"xltime\" in df.columns:\n",
    "                # A) Convert from Excel days -> naive UTC\n",
    "                df = df.with_columns(\n",
    "                    (\n",
    "                        (pl.col(\"xltime\") - EXCEL_EPOCH_OFFSET)\n",
    "                        * SECONDS_PER_DAY\n",
    "                        * 1_000_000_000\n",
    "                    )\n",
    "                    .cast(pl.Int64)\n",
    "                    .cast(pl.Datetime(\"ns\"))\n",
    "                    .alias(\"ny_timestamp\")  # weâ€™ll shift this below\n",
    "                ).drop(\"xltime\")\n",
    "\n",
    "                # B) Shift by 4 hours to approximate EDT\n",
    "                #    ny_timestamp = ny_timestamp - 4h\n",
    "                df = df.with_columns(\n",
    "                    (\n",
    "                        pl.col(\"ny_timestamp\").cast(pl.Int64)\n",
    "                        - (UTC_OFFSET_HOURS * 3600 * 1_000_000_000)\n",
    "                    )\n",
    "                    .cast(pl.Datetime(\"ns\"))\n",
    "                    .alias(\"ny_timestamp\")\n",
    "                )\n",
    "\n",
    "            # 3) Overwrite the same CSV\n",
    "            df.write_csv(csv_path)\n",
    "            print(f\"Overwrote {csv_path} with local 'ny_timestamp' column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(root, filename)\n",
    "\n",
    "            # 1) Read with Polars\n",
    "            df = pl.read_csv(csv_path)\n",
    "            \n",
    "            # 2) Drop utc_timestamp if present\n",
    "            if \"utc_timestamp\" in df.columns:\n",
    "                df = df.drop(\"utc_timestamp\")\n",
    "                \n",
    "                # 3) Overwrite the same CSV file\n",
    "                df.write_csv(csv_path)\n",
    "                print(f\"Overwrote {csv_path} without utc_timestamp.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
